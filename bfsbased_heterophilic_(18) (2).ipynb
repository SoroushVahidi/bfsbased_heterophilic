{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqGs0nK9M8Kq",
        "outputId": "35a4d703-fdb2-4650-d974-ac72841b86c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gurobipy in /usr/local/lib/python3.11/dist-packages (12.0.1)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (2.4.0)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (25.1.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gurobipy\n",
        "!pip install torch_geometric\n",
        "!pip install sortedcontainers\n",
        "\n",
        "#!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric\n",
        "!pip install scikit-optimize\n",
        "\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.metrics import pairwise_distances as sklearn_pairwise_distances\n",
        "import networkx as nx\n",
        " # Use \"cosine\" for cosine similarity\n",
        "import heapq\n",
        "\n",
        "\n",
        "os.environ[\"GRB_LICENSE_FILE\"] = \"gurobi (3).lic\"\n",
        "os.environ[\"GRB_WLSACCESSID\"] = \"f218200d-1f8d-4342-83f5-b7b2d9263751\"  # Replace with your actual WLSACCESSID\n",
        "os.environ[\"GRB_WLSSECRET\"] = \"528d596b-babc-4a1e-bda2-693c44f4f006\"  # Replace with your actual WLSSECRET\n",
        "os.environ[\"GRB_LICENSEID\"] = \"840285\"  # Replace with your actual LICENSEID\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from torch_geometric.datasets import WebKB, WikipediaNetwork, Actor\n",
        "\n",
        "from torch_geometric.datasets import FacebookPagePage\n",
        "\n",
        "from torch_geometric.datasets import SNAPDataset\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import networkx as nx\n",
        "from torch_geometric.datasets import WebKB\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sortedcontainers import SortedSet  # Ordered set\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "from torch_geometric.datasets import WikipediaNetwork\n",
        "from torch_geometric.datasets import Actor\n",
        "\n",
        "# Load the Film dataset (also known as Actor dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD28jxmhNmm5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZChMrfbyNoOn",
        "outputId": "08d6702a-da25-43fb-8285-b9d07427139f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Loaded  dataset with 183 nodes, 574 edges\n",
            "   - Features: 1703\n",
            "   - Number of Classes: 5\n",
            "   - Number of Components: 1\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import LINKXDataset\n",
        "#dataset = WebKB(root='data', name='Cornell')\n",
        "dataset = WebKB(root='data', name='texas')\n",
        "#dataset = WikipediaNetwork(root='data', name='chameleon')\n",
        "#dataset = WikipediaNetwork(root='data', name='squirrel')\n",
        "#dataset = WebKB(root='data', name='Wisconsin')\n",
        "#dataset = Actor(root='data/Film')\n",
        "root = 'data/'\n",
        "#dataset = LINKXDataset(root=root, name='penn94')\n",
        "data = dataset[0]\n",
        "data.edge_index = torch.cat([data.edge_index, data.edge_index.flip(0)], dim=1)\n",
        "\n",
        "# âœ… Remove duplicate edges\n",
        "data.edge_index = torch.unique(data.edge_index, dim=1)\n",
        "\n",
        "# âœ… Convert to NetworkX Graph to Find Components\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "components = list(nx.connected_components(G))\n",
        "num_components = len(components)\n",
        "\n",
        "print(f\"\\nâœ… Loaded  dataset with {data.num_nodes} nodes, {data.edge_index.shape[1]} edges\")\n",
        "print(f\"   - Features: {data.x.shape[1]}\")\n",
        "print(f\"   - Number of Classes: {len(set(data.y.numpy()))}\")\n",
        "print(f\"   - Number of Components: {num_components}\")\n",
        "\n",
        "# âœ… Set Training Percentage\n",
        "# train_percentage = 0.65\n",
        "\n",
        "# # âœ… Select Training Nodes\n",
        "# train_indices = []\n",
        "# for component in components:\n",
        "#     component = list(component)\n",
        "#     t = max(1, int(train_percentage * len(component)))\n",
        "#     sampled_nodes = np.random.choice(component, t, replace=False)\n",
        "#     train_indices.extend(sampled_nodes)\n",
        "\n",
        "# np.random.shuffle(train_indices)\n",
        "\n",
        "# # âœ… Initialize Prediction Labels\n",
        "# y_pred = np.full(data.num_nodes, -1)\n",
        "# for idx in train_indices:\n",
        "#     y_pred[idx] = data.y[idx].item()\n",
        "\n",
        "# # âœ… Compute Label Distribution\n",
        "# total_label_counts = Counter(data.y.numpy())\n",
        "# num_labels = len(set(data.y.numpy()))\n",
        "# alpha = 1  # Laplace smoothing\n",
        "\n",
        "# dataset_label_distribution = {\n",
        "#     lbl: (total_label_counts[lbl] + alpha) / (len(data.y) + alpha * num_labels)\n",
        "#     for lbl in total_label_counts.keys()\n",
        "# }\n",
        "\n",
        "# train_label_counts = Counter(y_pred[train_indices])\n",
        "\n",
        "# train_label_distribution = {\n",
        "#     lbl: (train_label_counts.get(lbl, 0) + alpha) / (len(train_indices) + alpha * num_labels)\n",
        "#     for lbl in dataset_label_distribution.keys()\n",
        "# }\n",
        "\n",
        "# # âœ… Precompute Neighbors for Each Node\n",
        "# A = torch.zeros((data.num_nodes, data.num_nodes))\n",
        "# A[data.edge_index[0], data.edge_index[1]] = 1\n",
        "# A = A.numpy()\n",
        "# neighbors_dict = {i: set(np.where(A[i] == 1)[0]) for i in range(data.num_nodes)}\n",
        "\n",
        "# # âœ… Track Instances Assigned to Each Label\n",
        "# class_instances = defaultdict(set)\n",
        "# for idx in train_indices:\n",
        "#     class_instances[data.y[idx].item()].add(idx)\n",
        "\n",
        "# # âœ… Ordered Set for Managing Unlabeled Vertices\n",
        "# ordered_set = SortedSet()\n",
        "# train_labeled_nodes = set(train_indices)\n",
        "\n",
        "# for node in range(data.num_nodes):\n",
        "#     if y_pred[node] == -1:  # Only process unlabeled nodes\n",
        "#         labeled_neighbors = [n for n in neighbors_dict[node] if y_pred[n] != -1]\n",
        "#         num_labeled_by_train = sum(1 for n in labeled_neighbors if n in train_labeled_nodes)\n",
        "#         num_labeled_by_propagation = len(labeled_neighbors) - num_labeled_by_train\n",
        "#         total_neighbors = len(neighbors_dict[node])\n",
        "\n",
        "#         if total_neighbors > 0:\n",
        "#             weighted_score = (num_labeled_by_propagation + 3 * num_labeled_by_train) / total_neighbors\n",
        "#         else:\n",
        "#             weighted_score = 0\n",
        "\n",
        "#         ordered_set.add((-weighted_score, node))\n",
        "\n",
        "# âœ… Iteratively Label the Most Constrained Nodes\n",
        "# while ordered_set:\n",
        "#     _, node = ordered_set.pop(0)\n",
        "\n",
        "#     if y_pred[node] != -1:\n",
        "#         continue\n",
        "\n",
        "#     # Get labeled neighbors\n",
        "#     neighbor_labels = [y_pred[n] for n in neighbors_dict[node] if y_pred[n] != -1]\n",
        "#     label_counts = Counter(neighbor_labels)\n",
        "\n",
        "#     # ðŸš¨ If this node has 0 labeled neighbors\n",
        "#     if not neighbor_labels:\n",
        "#         print(f\"\\nðŸš¨ STRANGE: Node {node} was chosen, but it has 0 labeled neighbors!\")\n",
        "#         break\n",
        "\n",
        "#     # Compute Neighbor Label Distribution\n",
        "#     neighbor_label_counts = Counter(neighbor_labels)\n",
        "#     total_labeled_neighbors = len(neighbor_labels)\n",
        "\n",
        "#     if total_labeled_neighbors > 0:\n",
        "#         neighbor_label_distribution = {\n",
        "#             lbl: neighbor_label_counts[lbl] / total_labeled_neighbors for lbl in neighbor_label_counts.keys()\n",
        "#         }\n",
        "#     else:\n",
        "#         neighbor_label_distribution = {lbl: 0 for lbl in dataset_label_distribution.keys()}\n",
        "\n",
        "    # âœ… Compute Feature Distance to Each Class\n",
        "    # feature_diffs = {}\n",
        "    # for lbl in dataset_label_distribution.keys():\n",
        "    #   if class_instances[lbl]:  # Only compute if we have instances of the label\n",
        "    #       instance_features = data.x[list(class_instances[lbl])]\n",
        "    #       node_feature = data.x[node].unsqueeze(0)\n",
        "\n",
        "    #       # Compute distances for each instance\n",
        "    #       distances = torch.norm(instance_features - node_feature, dim=1).tolist()\n",
        "\n",
        "    #       # Apply weight based on whether the instance is from training or predicted labels\n",
        "    #       weighted_distances = [\n",
        "    #           (3 * dist) if instance in train_indices else dist  # Give 3x weight if in training set\n",
        "    #           for instance, dist in zip(class_instances[lbl], distances)\n",
        "    #       ]\n",
        "\n",
        "    #       # Compute weighted average distance\n",
        "    #       avg_distance = sum(weighted_distances) / len(weighted_distances)\n",
        "    #       feature_diffs[lbl] = avg_distance\n",
        "    #   else:\n",
        "    #       feature_diffs[lbl] = 0  # No penalty if no instances exist\n",
        "\n",
        "#     feature_diffs = {}\n",
        "#     for lbl in dataset_label_distribution.keys():\n",
        "#         if class_instances[lbl]:  # Only compute if we have instances of the label\n",
        "#             instance_features = data.x[list(class_instances[lbl])]\n",
        "#             node_feature = data.x[node].unsqueeze(0)\n",
        "#            # print(len(instance_features),len(node_feature))\n",
        "#             avg_distance = torch.mean(torch.norm(instance_features - node_feature, dim=1)).item()\n",
        "#             feature_diffs[lbl] = avg_distance\n",
        "#         else:\n",
        "#             feature_diffs[lbl] = 0  # No penalty if no instances exist\n",
        "\n",
        "#     # âœ… Select the Best Label Based on the Adjusted Score\n",
        "#     best_label_candidates = []\n",
        "#     max_value = float('-inf')\n",
        "#     a1=0.5\n",
        "#     a2=-6\n",
        "#     a3=-2\n",
        "#     a4=1\n",
        "#     a5=3\n",
        "#     for lbl in dataset_label_distribution.keys():\n",
        "#         score = (\n",
        "#             a1*train_label_distribution.get(lbl, 0)\n",
        "#             +a2* neighbor_label_distribution.get(lbl, 0)\n",
        "#             +a3*feature_diffs[lbl]  # Penalize by average feature distance\n",
        "#         )\n",
        "#         print(lbl,score,)\n",
        "\n",
        "#         if score - max_value > 0.01:\n",
        "#             best_label_candidates = [lbl]\n",
        "#             max_value = score\n",
        "#         elif abs(score - max_value) <= 0.01:\n",
        "#             best_label_candidates.append(lbl)\n",
        "\n",
        "\n",
        "#     best_label = random.choice(best_label_candidates)\n",
        "#     y_pred[node] = best_label\n",
        "#     class_instances[best_label].add(node)  # Track newly labeled nodes\n",
        "#     neighbor_labels = [y_pred[n] for n in neighbors_dict[node] if y_pred[n] != -1]\n",
        "#     label_counts = Counter(neighbor_labels)\n",
        "\n",
        "#     print(f\"\\nðŸ”¹ Labelling Node {node} \")\n",
        "#     # âœ… Print Prediction Outcome\n",
        "#     true_label = data.y[node].item()\n",
        "#     correct = \"âœ…\" if best_label == true_label else \"âŒ\"\n",
        "#     print(f\"   -> Predicted Label: {best_label} | True Label: {true_label} {correct}\")\n",
        "\n",
        "#    # print(f\"   - Total Neighbors: {len(neighbors_dict[node])}\")\n",
        "#    # print(f\"   - Labeled Neighbors: {len(neighbor_labels)}\")\n",
        "#     for lbl, count in label_counts.items():\n",
        "#         print(f\"   - Class {lbl}: {count} neighbors\")\n",
        "#         # âœ… Update Labeled Neighbor Score for Unlabeled Neighbors\n",
        "#         for neighbor in neighbors_dict[node]:\n",
        "#             if y_pred[neighbor] == -1:\n",
        "#                 old_entry = next((entry for entry in ordered_set if entry[1] == neighbor), None)\n",
        "\n",
        "#                 if old_entry:\n",
        "#                     ordered_set.discard(old_entry)\n",
        "\n",
        "#                 num_labeled_by_train = sum(1 for n in neighbors_dict[neighbor] if y_pred[n] != -1 and n in train_labeled_nodes)\n",
        "#                 num_labeled_by_propagation = sum(1 for n in neighbors_dict[neighbor] if y_pred[n] != -1 and n not in train_labeled_nodes)\n",
        "#                 total_neighbors = len(neighbors_dict[neighbor])\n",
        "\n",
        "#                 new_weighted_score = (a4*num_labeled_by_propagation + a5 * num_labeled_by_train) / total_neighbors if total_neighbors > 0 else 0\n",
        "\n",
        "#                 ordered_set.add((-new_weighted_score, neighbor))\n",
        "#     print(\"\\n\\n\\n\")\n",
        "\n",
        "# # âœ… Evaluate Accuracy\n",
        "# test_indices = [i for i in range(data.num_nodes) if i not in train_indices]\n",
        "# y_true = data.y[test_indices]\n",
        "# y_pred_test = y_pred[test_indices]\n",
        "\n",
        "# valid_idx = [i for i in range(len(y_pred_test)) if y_pred_test[i] != -1]\n",
        "# if valid_idx:\n",
        "#     final_accuracy = accuracy_score(y_true[valid_idx], y_pred_test[valid_idx])\n",
        "#     print(f\"\\nðŸŽ¯ Label Propagation Accuracy: {final_accuracy:.4f}\")\n",
        "# else:\n",
        "#     print(\"\\nâŒ No valid predictions were made!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ru9be0GZSBHA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from sortedcontainers import SortedSet\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from sortedcontainers import SortedSet\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predictclass(data, train_indices, test_indices, a1, a2, a3, a4, a5, a6, a7):\n",
        "   # print(f\"data.x shape: {data.x.shape}\")\n",
        "    data.edge_index = torch.cat([data.edge_index, data.edge_index.flip(0)], dim=1)\n",
        "    train_indices = list(train_indices)\n",
        "    test_indices = list(test_indices)\n",
        "\n",
        "    # âœ… Remove duplicate edges\n",
        "    data.edge_index = torch.unique(data.edge_index, dim=1)\n",
        "\n",
        "    # âœ… Initialize Prediction Labels\n",
        "    y_pred = np.full(data.num_nodes, -1, dtype=int)\n",
        "    for idx in train_indices:\n",
        "        y_pred[idx] = data.y[idx].item()\n",
        "\n",
        "    # âœ… Compute Label Distribution\n",
        "    total_label_counts = Counter(data.y.numpy())\n",
        "    num_labels = len(set(data.y.numpy()))\n",
        "    alpha = 1  # Laplace smoothing\n",
        "\n",
        "    dataset_label_distribution = {\n",
        "        lbl: (total_label_counts[lbl] + alpha) / (len(data.y) + alpha * num_labels)\n",
        "        for lbl in total_label_counts.keys()\n",
        "    }\n",
        "\n",
        "    train_label_counts = Counter(y_pred[train_indices])\n",
        "\n",
        "    train_label_distribution = {\n",
        "        lbl: (train_label_counts.get(lbl, 0) + alpha) / (len(train_indices) + alpha * num_labels)\n",
        "        for lbl in dataset_label_distribution.keys()\n",
        "    }\n",
        "\n",
        "    # âœ… Precompute Neighbors for Each Node\n",
        "    A = torch.zeros((data.num_nodes, data.num_nodes))\n",
        "    A[data.edge_index[0], data.edge_index[1]] = 1\n",
        "    neighbors_dict = {i: set(np.where(A[i] == 1)[0]) for i in range(data.num_nodes)}\n",
        "\n",
        "    # âœ… Track Instances Assigned to Each Label\n",
        "    class_instances = defaultdict(set)\n",
        "    for idx in train_indices:\n",
        "        class_instances[data.y[idx].item()].add(idx)\n",
        "\n",
        "    # âœ… Initialize D Table (Similarity Table)\n",
        "    D = {i: {lbl: 0.0 for lbl in dataset_label_distribution.keys()} for i in range(data.num_nodes)}\n",
        "    class_counts = {lbl: 0 for lbl in dataset_label_distribution.keys()}\n",
        "\n",
        "    # âœ… Compute Average Feature Vector Once Per Label\n",
        "    avg_features = {}\n",
        "    for lbl in dataset_label_distribution.keys():\n",
        "        if class_instances[lbl]:\n",
        "            instance_list = list(map(int, class_instances[lbl]))  # Ensure list of Python ints\n",
        "            instance_list = torch.tensor(instance_list, dtype=torch.long)  # Convert to PyTorch tensor\n",
        "\n",
        "            instance_features = data.x[instance_list]\n",
        "            instance_weights = torch.tensor(\n",
        "                [(1 - a7) if inst in train_indices else a7 for inst in instance_list],\n",
        "                dtype=torch.float,\n",
        "                device=data.x.device\n",
        "            )\n",
        "            instance_weights /= instance_weights.sum()\n",
        "            avg_features[lbl] = torch.sum(instance_features * instance_weights.view(-1, 1), dim=0, keepdim=True)\n",
        "\n",
        "    # âœ… Compute Similarity for Each Unlabeled Node\n",
        "    for lbl in dataset_label_distribution.keys():\n",
        "        if lbl in avg_features:\n",
        "            for i in range(data.num_nodes):\n",
        "                similarity = F.cosine_similarity(data.x[i].unsqueeze(0), avg_features[lbl]).item()\n",
        "                D[i][lbl] = (similarity + 1) / 2\n",
        "\n",
        "    # âœ… Ordered Set for Managing Unlabeled Vertices\n",
        "    ordered_set = SortedSet()\n",
        "    train_labeled_nodes = set(train_indices)\n",
        "    labeled_nodes_count = 0\n",
        "\n",
        "    for node in range(data.num_nodes):\n",
        "        if y_pred[node] == -1:\n",
        "            labeled_neighbors = [n for n in neighbors_dict[node] if y_pred[n] != -1]\n",
        "            num_labeled_by_train = sum(1 for n in labeled_neighbors if n in train_labeled_nodes)\n",
        "            num_labeled_by_propagation = len(labeled_neighbors) - num_labeled_by_train\n",
        "            total_neighbors = len(neighbors_dict[node])\n",
        "            weighted_score = (a4 * num_labeled_by_propagation + a5 * num_labeled_by_train) / total_neighbors if total_neighbors > 0 else 0\n",
        "            similarity_score = max(D[node].values()) if D[node] else 0\n",
        "            ordered_set.add((-weighted_score - a6 * similarity_score, node))\n",
        "\n",
        "    while ordered_set:\n",
        "        _, node = ordered_set.pop(0)\n",
        "\n",
        "        if y_pred[node] != -1:\n",
        "            continue\n",
        "            print(\"This node is already labeled. Something is wrong.\")\n",
        "\n",
        "        neighbor_labels = [y_pred[n] for n in neighbors_dict[node] if y_pred[n] != -1]\n",
        "\n",
        "\n",
        "        neighbor_label_counts = Counter(neighbor_labels)\n",
        "        total_labeled_neighbors = len(neighbor_labels)\n",
        "        neighbor_label_distribution = {\n",
        "            lbl: neighbor_label_counts[lbl] / total_labeled_neighbors for lbl in neighbor_label_counts.keys()\n",
        "        } if total_labeled_neighbors > 0 else {lbl: 0 for lbl in dataset_label_distribution.keys()}\n",
        "\n",
        "        label_similarity = {lbl: D[node][lbl] for lbl in dataset_label_distribution.keys()}\n",
        "\n",
        "        best_label_candidates = []\n",
        "        max_value = float('-inf')\n",
        "\n",
        "        for lbl in dataset_label_distribution.keys():\n",
        "            score = (\n",
        "                a1 * train_label_distribution.get(lbl, 0)\n",
        "                + a2 * neighbor_label_distribution.get(lbl, 0)\n",
        "                + a3 * label_similarity[lbl]\n",
        "            )\n",
        "\n",
        "            if score - max_value > 0.01:\n",
        "                best_label_candidates = [lbl]\n",
        "                max_value = score\n",
        "            elif abs(score - max_value) <= 0.01:\n",
        "                best_label_candidates.append(lbl)\n",
        "\n",
        "        best_label = random.choice(best_label_candidates)\n",
        "        y_pred[node] = best_label\n",
        "        class_instances[best_label].add(node)\n",
        "        class_counts[best_label] += 1\n",
        "        labeled_nodes_count += 1\n",
        "\n",
        "\n",
        "\n",
        "        if labeled_nodes_count % (data.num_nodes // 5) == 0 == 0:\n",
        "            avg_features = {}\n",
        "            for lbl in dataset_label_distribution.keys():\n",
        "                if class_instances[lbl]:\n",
        "                    instance_list = list(class_instances[lbl])\n",
        "                    instance_list = list(map(int, class_instances[lbl]))  # Ensure list of Python ints\n",
        "                    instance_list = torch.tensor(instance_list, dtype=torch.long)  # Convert to PyTorch tensor\n",
        "\n",
        "                    instance_features = data.x[instance_list]  # Now indexing should work\n",
        "\n",
        "                    instance_weights = torch.tensor(\n",
        "                        [(1 - a7) if inst in train_indices else a7 for inst in instance_list],\n",
        "                        dtype=torch.float,\n",
        "                        device=data.x.device\n",
        "                    )\n",
        "                    instance_weights /= instance_weights.sum()\n",
        "                    avg_features[lbl] = torch.sum(instance_features * instance_weights.view(-1, 1), dim=0, keepdim=True)\n",
        "\n",
        "            for lbl in dataset_label_distribution.keys():\n",
        "                if lbl in avg_features:\n",
        "                    for i in range(data.num_nodes):\n",
        "                        similarity = F.cosine_similarity(data.x[i].unsqueeze(0), avg_features[lbl]).item()\n",
        "                        D[i][lbl] = (similarity + 1) / 2\n",
        "\n",
        "            # âœ… Update Scores in Ordered Set\n",
        "            new_ordered_set = SortedSet()\n",
        "            for node in range(data.num_nodes):\n",
        "                if y_pred[node] == -1:\n",
        "                    labeled_neighbors = [n for n in neighbors_dict[node] if y_pred[n] != -1]\n",
        "                    num_labeled_by_train = sum(1 for n in labeled_neighbors if n in train_labeled_nodes)\n",
        "                    num_labeled_by_propagation = len(labeled_neighbors) - num_labeled_by_train\n",
        "                    total_neighbors = len(neighbors_dict[node])\n",
        "                    weighted_score = (a4 * num_labeled_by_propagation + a5 * num_labeled_by_train) / total_neighbors if total_neighbors > 0 else 0\n",
        "                    similarity_score = max(D[node].values()) if D[node] else 0\n",
        "                    new_ordered_set.add((-weighted_score - a6 * similarity_score, node))\n",
        "            ordered_set = new_ordered_set\n",
        "\n",
        "    return torch.tensor(y_pred[test_indices], dtype=torch.float), accuracy_score(data.y[test_indices], y_pred[test_indices])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqUQNrZolOwy",
        "outputId": "7d136a97-0f6f-45a6-c191-28bf63e18f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: 33 elements\n",
            "Class 1: 1 elements\n",
            "Class 2: 18 elements\n",
            "Class 3: 101 elements\n",
            "Class 4: 30 elements\n"
          ]
        }
      ],
      "source": [
        "label_shift = -data.y.min().item() if data.y.min().item() < 0 else 0\n",
        "y_shifted = data.y + label_shift\n",
        "\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "\n",
        "\n",
        "# Count occurrences of each class label\n",
        "class_counts = torch.bincount(y_shifted)\n",
        "\n",
        "# Print the number of elements in each class\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    print(f\"Class {class_id}: {count} elements\")\n",
        "a1=0.5\n",
        "a2=-6\n",
        "a3=-2\n",
        "a4=1\n",
        "a5=3\n",
        "a6=1\n",
        "\n",
        "indices = np.arange(data.num_nodes)  # NumPy array of indices [0, 1, 2, ..., num_nodes-1]\n",
        "np.random.shuffle(indices)\n",
        "train_idx, test_idx = train_test_split(indices, test_size=0.6)\n",
        "\n",
        "# Further split train_idx into train and validation sets (e.g., 20% of train goes to validation)\n",
        "train_idx, val_idx = train_test_split(train_idx, test_size=0.5)\n",
        "test_indices=test_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u88m8qZfUdX-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hsMOjg0RUedb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "88eDCyKNUf57"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xWZxuxXlygBh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.model_selection import KFold\n",
        "import warnings\n",
        "warnings.simplefilter(\"error\", RuntimeWarning)  # Convert warnings to errors\n",
        "#dataset = WebKB(root='data', name='Cornell')\n",
        "#dataset = WebKB(root='data', name='texas')\n",
        "#dataset = WikipediaNetwork(root='data', name='chameleon')\n",
        "#dataset = WikipediaNetwork(root='data', name='squirrel')\n",
        "#dataset = WebKB(root='data', name='Wisconsin')\n",
        "\n",
        "\n",
        "\n",
        "def random_search(model, data, num_samples=50, num_splits=5):\n",
        "    \"\"\"\n",
        "    Perform random search for hyperparameter tuning while evaluating each set\n",
        "    of hyperparameters across multiple train-test splits.\n",
        "\n",
        "    Arguments:\n",
        "        model: Function to predict classes.\n",
        "        data: Dataset object.\n",
        "        num_samples: Number of random search samples (default: 120).\n",
        "        num_splits: Number of different train-test splits to evaluate each hyperparameter set.\n",
        "\n",
        "    Returns:\n",
        "        best_params: The best hyperparameters found based on average accuracy.\n",
        "        best_score: The highest average accuracy obtained.\n",
        "    \"\"\"\n",
        "    best_params = None\n",
        "    best_score = float('-inf')\n",
        "    indices = np.arange(data.num_nodes)\n",
        "    maxi=1\n",
        "    for _ in range(num_samples):\n",
        "        print(\"step number \",_)\n",
        "        # Generate random hyperparameters\n",
        "        a1 = random.uniform(0, maxi)     # a1 must be positive\n",
        "        a2 = random.uniform(-maxi, -0.01)  # a2 must be negative\n",
        "        a3 = random.uniform(0, maxi)  # a3 must be negative\n",
        "        a5 = random.uniform(0, maxi)     # a5 must be positive\n",
        "        a4 = random.uniform(0, a5)    # a4 must be positive and a4 <= a5\n",
        "        a6 = random.uniform(0, maxi)\n",
        "        a7 = random.uniform(0, maxi/2)\n",
        "       # a8 = random.uniform(0, a7)\n",
        "        params = [a1, a2, a3, a4, a5, a6,a7]\n",
        "\n",
        "        fold_scores = []\n",
        "        kf = KFold(n_splits=num_splits, shuffle=True, random_state=None)  # Different train-test splits\n",
        "\n",
        "        for train_idx, val_idx in kf.split(indices):\n",
        "            _, score = model(data, train_idx, val_idx, *params)\n",
        "            fold_scores.append(score)\n",
        "\n",
        "        avg_score = np.mean(fold_scores)\n",
        "\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, best_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initial guesses for a1, a2, a3, a4, a5\n",
        "#initial_params = [0.5,-5,-2,1,3]\n",
        "\n",
        "# Bounds for parameters\n",
        " # Adjust bounds as needed\n",
        "\n",
        "# Optimize using scipy\n",
        "# result,best_val = random_search(predictclass,data,num_samples=15,num_splits=5)\n",
        "# #print(result)\n",
        "# # Get best parameters\n",
        "# best_a1, best_a2, best_a3, best_a4, best_a5, best_a6,best_a7 = result[0],result[1],result[2],result[3],result[4],result[5],result[6]\n",
        "\n",
        "# # Evaluate on test set\n",
        "\n",
        "\n",
        "# # Print results\n",
        "# print(\"best validation by random search:\", best_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NqOUOkBf6ACE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from skopt.space import Real\n",
        "from skopt import gp_minimize\n",
        "def bayesian_optimization(model, data, num_calls=10, num_splits=5, initial_params=None):\n",
        "    \"\"\"\n",
        "    Bayesian Optimization to find best hyperparameters (a1, a2, a3, a4, a5, a6)\n",
        "    that maximize the validation accuracy while evaluating across multiple train-test splits.\n",
        "\n",
        "    Arguments:\n",
        "        model: Function that takes (data, train_idx, val_idx, a1, a2, a3, a4, a5, a6) and returns (loss, accuracy)\n",
        "        data: Dataset object.\n",
        "        num_calls: Number of function evaluations (default: 30).\n",
        "        num_splits: Number of different train-test splits to evaluate each hyperparameter set.\n",
        "        initial_params: Optional initial solution from random search.\n",
        "\n",
        "    Returns:\n",
        "        best_params: The best found values for [a1, a2, a3, a4, a5, a6].\n",
        "        best_accuracy: The best validation accuracy found.\n",
        "    \"\"\"\n",
        "    maxi=1\n",
        "    search_space = [\n",
        "        Real(0, maxi, name=\"a1\"),        # a1 must be positive\n",
        "        Real(-maxi, -0.01, name=\"a2\"),   # a2 must be negative\n",
        "        Real(0, maxi, name=\"a3\"),   # a3 must be negative\n",
        "        Real(0, maxi, name=\"a4\"),        # a4 must be positive and a4 â‰¤ a5\n",
        "        Real(0, maxi, name=\"a5\"),        # a5 must be positive\n",
        "        Real(0, maxi, name=\"a6\"),\n",
        "        Real(0, maxi/2, name=\"a7\") , # a6 must be positive\n",
        "       # Real(0, maxi, name=\"a8\")\n",
        "    ]\n",
        "\n",
        "    indices = np.arange(data.num_nodes)\n",
        "\n",
        "    def objective(params):\n",
        "        a1, a2, a3, a4, a5, a6,a7 = params\n",
        "        a4, a5 = min(a4, a5), max(a4, a5)  # Ensure a4 â‰¤ a5\n",
        "       # a8, a7 = min(a8, a7), max(a8, a7)\n",
        "        fold_scores = []\n",
        "        kf = KFold(n_splits=num_splits, shuffle=True, random_state=None)  # Different train-test splits\n",
        "\n",
        "        for train_idx, val_idx in kf.split(indices):\n",
        "            _, score = model(data, train_idx, val_idx, a1, a2, a3, a4, a5, a6,a7)\n",
        "            fold_scores.append(score)\n",
        "\n",
        "        avg_score = np.mean(fold_scores)\n",
        "        return -avg_score  # Since `gp_minimize` minimizes the function, we negate accuracy\n",
        "\n",
        "    if initial_params is not None:\n",
        "        print(\"initial_params=\",initial_params)\n",
        "        initial_params[3], initial_params[4] = min(initial_params[3], initial_params[4]), max(initial_params[3], initial_params[4])\n",
        "\n",
        "        #initial_params[7], initial_params[6] = min(initial_params[7], initial_params[6]), max(initial_params[7], initial_params[6])\n",
        "        result = gp_minimize(objective, search_space, n_calls=num_calls, x0=[initial_params], random_state=None)\n",
        "    else:\n",
        "        result = gp_minimize(objective, search_space, n_calls=num_calls, random_state=None)\n",
        "\n",
        "    best_a1, best_a2, best_a3, best_a4, best_a5, best_a6,best_a7 = result.x\n",
        "    best_a4, best_a5 = min(best_a4, best_a5), max(best_a4, best_a5)  # Ensure a4 â‰¤ a5\n",
        "    #best_a8, best_a7 = min(best_a8, best_a7), max(best_a8, best_a7)\n",
        "    best_accuracy = -result.fun\n",
        "\n",
        "    best_params = [best_a1, best_a2, best_a3, best_a4, best_a5, best_a6,best_a7]\n",
        "\n",
        "    return best_params, best_accuracy\n",
        "\n",
        "# Example Usage:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FH9DxCcGSpLc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "2bcbf3cd-8c5b-4107-8745-abbefb3d8b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”„ Iteration 1/10\n",
            "step number  0\n",
            "step number  1\n",
            "step number  2\n",
            "step number  3\n",
            "step number  4\n",
            "step number  5\n",
            "step number  6\n",
            "step number  7\n",
            "step number  8\n",
            "step number  9\n",
            "initial_params= [0.24837610892904338, -0.7768388163753946, 0.564232634605594, 0.16373665057686304, 0.20290017081123157, 0.6036656315488492, 0.29718760078993434]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f00a63fbe28c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mbest_params_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mbest_params_bayes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_bayes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayesian_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_params_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_params_bayes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbest_val_bayes\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbest_val_random\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbest_params_random\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7319b82ee9bf>\u001b[0m in \u001b[0;36mbayesian_optimization\u001b[0;34m(model, data, num_calls, num_splits, initial_params)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#initial_params[7], initial_params[6] = min(initial_params[7], initial_params[6]), max(initial_params[7], initial_params[6])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_minimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minitial_params\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_minimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[1;32m    279\u001b[0m         )\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     return base_minimize(\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/skopt/optimizer/optimizer.py\u001b[0m in \u001b[0;36mtell\u001b[0;34m(self, x, y, fit)\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/skopt/optimizer/optimizer.py\u001b[0m in \u001b[0;36m_tell\u001b[0;34m(self, x, y, fit)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m                 \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"next_xs_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macq_func\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gp_hedge\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/skopt/learning/gaussian_process/gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_level_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fixed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 )\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mtheta_initial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     optima.append(\n\u001b[0;32m--> 326\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constrained_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m                     )\n\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Select result from run with minimal (negative) log-marginal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36m_constrained_optimization\u001b[0;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_constrained_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fmin_l_bfgs_b\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             opt_res = scipy.optimize.minimize(\n\u001b[0m\u001b[1;32m    654\u001b[0m                 \u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                 \u001b[0minitial_theta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    729\u001b[0m                                  **options)\n\u001b[1;32m    730\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    732\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    733\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lowest_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lowest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(theta, eval_gradient)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                     lml, grad = self.log_marginal_likelihood(\n\u001b[0m\u001b[1;32m    299\u001b[0m                         \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone_kernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[0;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;31m#     )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mlog_likelihood_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ik,ik->k\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0mlog_likelihood_dims\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0mlog_likelihood_dims\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# the log likehood is sum-up across the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     50\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     51\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "test_accuracies = []\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 10\n",
        "for run in range(num_iterations):\n",
        "    print(f\"\\nðŸ”„ Iteration {run + 1}/{num_iterations}\")\n",
        "\n",
        "    indices = np.arange(data.num_nodes)\n",
        "\n",
        "# Step 1: Split into 80% train+validation and 20% test\n",
        "    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=run, shuffle=True)\n",
        "\n",
        "\n",
        "    train_idx = torch.tensor(train_idx, dtype=torch.long).contiguous()\n",
        "\n",
        "    test_idx = torch.tensor(test_idx, dtype=torch.long).contiguous()\n",
        "\n",
        "\n",
        "    # Now call subgraph()\n",
        "    train_data = data.subgraph(train_idx)\n",
        "    test_data = data.subgraph(test_idx)\n",
        "\n",
        "\n",
        "\n",
        "    # Convert indices to PyTorch tensors for PyG subgraph function\n",
        "    #train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
        "    #test_idx = torch.tensor(test_idx, dtype=torch.long)\n",
        "\n",
        "\n",
        "    # Step 2: Split train+validation into 75% train and 25% validation\n",
        "    #train_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=run)\n",
        "    # (0.25 * 80%) = 20% of total, so remaining 60% is train\n",
        "\n",
        "    # Now we have:\n",
        "    # - train_idx (60% of total)\n",
        "    # - val_idx (20% of total)\n",
        "    # - test_idx (20% of total)\n",
        "\n",
        "    # Run hyperparameter tuning\n",
        "    best_params_random, best_val_random = random_search(predictclass, train_data, num_samples=10, num_splits=5)\n",
        "\n",
        "    best_params_bayes, best_val_bayes = bayesian_optimization(predictclass, train_data, num_calls=20, initial_params=best_params_random)\n",
        "\n",
        "    best_params = best_params_bayes if best_val_bayes >= best_val_random else best_params_random\n",
        "\n",
        "    # Extract optimized hyperparameters\n",
        "    best_a1, best_a2, best_a3, best_a4, best_a5, best_a6, best_a7 = best_params\n",
        "    #print(\"best params found\")\n",
        "    #print(len(data),len(train_idx),len(test_idx))\n",
        "    #print(data)\n",
        "    # Run predictclass with best parameters on test set\n",
        "    _, final_test_accuracy = predictclass(data, train_idx, test_idx, best_a1, best_a2, best_a3, best_a4, best_a5, best_a6, best_a7)\n",
        "\n",
        "    # Store the result\n",
        "    test_accuracies.append(final_test_accuracy)\n",
        "\n",
        "    print(f\"ðŸŽ¯ Test Accuracy for iteration {run + 1}: {final_test_accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbhUt_lv1Vh_"
      },
      "outputs": [],
      "source": [
        "mean_accuracy = np.mean(test_accuracies)\n",
        "std_accuracy = np.std(test_accuracies)\n",
        "\n",
        "print(f\"Mean: {mean_accuracy:.4f}, Std: {std_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c_sXm_FtUhXF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch.nn import Linear, Sequential, ReLU, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class HighPassConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, amp=0.5, **kwargs):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(**kwargs)\n",
        "        self.amp = amp\n",
        "        self.lin = Linear(in_channels, out_channels)\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin.reset_parameters()\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        x = self.lin(x)\n",
        "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
        "        out = self.amp * x - out\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, edge_weight):\n",
        "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "class Augmenter(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, amp=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = HighPassConv(in_dim, hidden_dim, amp=amp)\n",
        "        self.conv2 = HighPassConv(hidden_dim, hidden_dim, amp=amp)\n",
        "        self.mlp_edge_model = Sequential(\n",
        "            Dropout(0.5),\n",
        "            Linear(hidden_dim, hidden_dim * 2),\n",
        "            ReLU(),\n",
        "            Dropout(0.5),\n",
        "            Linear(hidden_dim * 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        src, dst = edge_index[0], edge_index[1]\n",
        "        edge_emb = x[src] + x[dst]\n",
        "        return torch.sigmoid(self.mlp_edge_model(edge_emb))\n",
        "\n",
        "class HeterophilicNodeClassifier(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, num_classes, test_indices, initial_preds=None):\n",
        "        super().__init__()\n",
        "        self.augmenter = Augmenter(in_dim, hidden_dim)\n",
        "        self.gnn_high = HighPassConv(in_dim, hidden_dim)\n",
        "        self.offset_mlp = Sequential(\n",
        "            Linear(in_dim, hidden_dim),\n",
        "            ReLU(),\n",
        "            Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        self.final_mlp = Sequential(\n",
        "            Linear(hidden_dim, hidden_dim),\n",
        "            ReLU(),\n",
        "            Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.test_indices = test_indices  # Store test indices\n",
        "\n",
        "        # Store initial predictions but only for test indices (not trained)\n",
        "        if initial_preds is not None:\n",
        "            self.initial_preds = F.one_hot(initial_preds.long(), num_classes=num_classes).float()\n",
        "        else:\n",
        "            self.initial_preds = None\n",
        "\n",
        "    def forward(self, x, edge_index, train_indices):\n",
        "        edge_weights = self.augmenter(x, edge_index)\n",
        "        h = self.gnn_high(x, edge_index, edge_weights)\n",
        "        offset = self.offset_mlp(x)\n",
        "        logits = self.final_mlp(h + offset)\n",
        "\n",
        "        # If initial_preds exists, add it **only for test nodes**\n",
        "        if self.initial_preds is not None:\n",
        "            logits[self.test_indices] = logits[self.test_indices] + self.initial_preds\n",
        "\n",
        "        return F.log_softmax(logits, dim=1)\n",
        "\n",
        "# Training and Evaluation Code\n",
        "def train_and_evaluate(model, data, train_indices, test_indices, initial_preds, epochs=2000, lr=0.01):\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(data.x, data.edge_index, train_indices)\n",
        "        loss = criterion(logits[train_indices], data.y[train_indices])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data.x, data.edge_index, train_indices)\n",
        "        predicted_classes = torch.argmax(logits, dim=1)\n",
        "        test_predictions = predicted_classes[test_indices]\n",
        "        final_accuracy = accuracy_score(data.y[test_indices].cpu().numpy(), test_predictions.cpu().numpy())\n",
        "\n",
        "    return final_accuracy, test_predictions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtC0NBH7UhKO"
      },
      "outputs": [],
      "source": [
        "# Compute initial test predictions\n",
        "initial_preds, initial_accuracy = predictclass(data, train_idx, test_idx, best_a1, best_a2, best_a3, best_a4, best_a5,best_a6)\n",
        "\n",
        "# Ensure `initial_preds` is a tensor and on the correct device\n",
        "initial_preds = initial_preds.clone().detach()  # Ensures a proper detached copy\n",
        "\n",
        "\n",
        "print(f\"ðŸ“Š Initial Classification Accuracy: {initial_accuracy:.4f}\")\n",
        "\n",
        "# Create the model\n",
        "model = HeterophilicNodeClassifier(\n",
        "    in_dim=data.x.shape[1],\n",
        "    hidden_dim=32,\n",
        "    num_classes=len(set(data.y.tolist())),test_indices=test_indices, initial_preds=initial_preds\n",
        ").to(data.x.device)  # Ensure model is on the correct device\n",
        "\n",
        "# ðŸ”„ Train the model before inference\n",
        "final_accuracy_before_djgnn, test_predictions_before_djgnn = train_and_evaluate(\n",
        "    model, data, train_idx, test_idx, initial_preds, epochs=100, lr=0.01\n",
        ")\n",
        "\n",
        "print(len(test_predictions_before_djgnn),len(test_idx))\n",
        "\n",
        "# âœ… Model evaluation after training\n",
        "model.eval()\n",
        "#with torch.no_grad():\n",
        " #   logits = model(data.x, data.edge_index, train_idx, test_idx, initial_test_preds=initial_preds)\n",
        " #   predicted_classes = torch.argmax(logits, dim=1)\n",
        "\n",
        "\n",
        "print(f\"ðŸŽ¯  Classification Accuracy after running kdd2023: {final_accuracy_before_djgnn:.4f}\")\n",
        "#print(test_predictions)\n",
        "#print(len(initial_preds))\n",
        "#print(len(test_predictions))\n",
        "\n",
        "#print(len(val_idx))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracies = []\n",
        "strategy_used = []\n",
        "\n",
        "num_iterations = 10\n",
        "\n",
        "for run in range(num_iterations):\n",
        "    print(f\"\\nðŸ”„ Iteration {run + 1}/{num_iterations}\")\n",
        "\n",
        "    indices = np.arange(data.num_nodes)\n",
        "    train_val_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=run, shuffle=True)\n",
        "    train_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=run)  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "    train_idx = torch.tensor(train_idx, dtype=torch.long).contiguous()\n",
        "    val_idx = torch.tensor(val_idx, dtype=torch.long).contiguous()\n",
        "    test_idx = torch.tensor(test_idx, dtype=torch.long).contiguous()\n",
        "\n",
        "    train_data = data.subgraph(train_idx)\n",
        "    val_data = data.subgraph(val_idx)\n",
        "\n",
        "    # Step 1: Hyperparameter tuning on training data\n",
        "    best_params_random, best_val_random = random_search(predictclass, train_data, num_samples=15, num_splits=5)\n",
        "    best_params_bayes, best_val_bayes = bayesian_optimization(predictclass, train_data, num_calls=25, initial_params=best_params_random)\n",
        "\n",
        "    best_params = best_params_bayes if best_val_bayes >= best_val_random else best_params_random\n",
        "    best_a1, best_a2, best_a3, best_a4, best_a5, best_a6, best_a7 = best_params\n",
        "\n",
        "    # Step 2: Run original model on validation set\n",
        "    _, val_accuracy_original = predictclass(data, train_idx, val_idx, best_a1, best_a2, best_a3, best_a4, best_a5, best_a6, best_a7)\n",
        "    print(f\"ðŸ“Š Validation Accuracy (Original Model): {val_accuracy_original:.4f}\")\n",
        "\n",
        "    # Step 3: Compute initial preds to feed into kdd2023 model\n",
        "    initial_preds, _ = predictclass(data, train_idx, val_idx, best_a1, best_a2, best_a3, best_a4, best_a5, best_a6, best_a7)\n",
        "    initial_preds = initial_preds.clone().detach()\n",
        "\n",
        "    # Step 4: Define and train kdd2023 model\n",
        "    model = HeterophilicNodeClassifier(\n",
        "        in_dim=data.x.shape[1],\n",
        "        hidden_dim=32,\n",
        "        num_classes=len(set(data.y.tolist())),\n",
        "        test_indices=val_idx,\n",
        "        initial_preds=initial_preds\n",
        "    ).to(data.x.device)\n",
        "\n",
        "    val_accuracy_kdd2023, _ = train_and_evaluate(model, data, train_idx, val_idx, initial_preds, epochs=100, lr=0.01)\n",
        "    print(f\"ðŸ§  Validation Accuracy (kdd2023 Model): {val_accuracy_kdd2023:.4f}\")\n",
        "\n",
        "    # Step 5: Choose better strategy based on validation accuracy\n",
        "    if val_accuracy_kdd2023 > val_accuracy_original:\n",
        "        chosen_strategy = \"Used kdd2023\"\n",
        "        print(\"âœ… Using kdd2023 model on test set.\")\n",
        "\n",
        "        # Recompute initial_preds for test\n",
        "        initial_preds_test, _ = predictclass(data, train_idx, test_idx, best_a1, best_a2, best_a3, best_a4, best_a5, best_a6, best_a7)\n",
        "        initial_preds_test = initial_preds_test.clone().detach()\n",
        "\n",
        "        # Recreate model with test_idx and initial_preds_test\n",
        "        model = HeterophilicNodeClassifier(\n",
        "            in_dim=data.x.shape[1],\n",
        "            hidden_dim=32,\n",
        "            num_classes=len(set(data.y.tolist())),\n",
        "            test_indices=test_idx,\n",
        "            initial_preds=initial_preds_test\n",
        "        ).to(data.x.device)\n",
        "\n",
        "        test_accuracy, _ = train_and_evaluate(model, data, train_idx, test_idx, initial_preds_test, epochs=100, lr=0.01)\n",
        "\n",
        "    else:\n",
        "        chosen_strategy = \"Skipped kdd2023\"\n",
        "        print(\"ðŸš« Skipping kdd2023. Using original model on test set.\")\n",
        "\n",
        "        _, test_accuracy = predictclass(data, train_idx, test_idx, best_a1, best_a2, best_a3, best_a4, best_a5, best_a6, best_a7)\n",
        "\n",
        "    # Record results\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    strategy_used.append(chosen_strategy)\n",
        "\n",
        "    print(f\"ðŸŽ¯ Final Test Accuracy: {test_accuracy:.4f} | Strategy: {chosen_strategy}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\nðŸ“Š Summary of Results:\")\n",
        "for i in range(num_iterations):\n",
        "    print(f\"Iteration {i+1}: Test Accuracy = {test_accuracies[i]:.4f}, Strategy = {strategy_used[i]}\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Average Test Accuracy: {sum(test_accuracies) / len(test_accuracies):.4f}\")\n",
        "\n",
        "import numpy as np  # Make sure this is at the top if not already imported\n",
        "\n",
        "print(f\"ðŸ“‰ Standard Deviation of Test Accuracies: {np.std(test_accuracies):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L5nUfvoN21I",
        "outputId": "02b9ca27-9eed-4f54-84f7-83e4f1c74ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”„ Iteration 1/10\n",
            "step number  0\n",
            "step number  1\n",
            "step number  2\n",
            "step number  3\n",
            "step number  4\n",
            "step number  5\n",
            "step number  6\n",
            "step number  7\n",
            "step number  8\n",
            "step number  9\n",
            "step number  10\n",
            "step number  11\n",
            "step number  12\n",
            "step number  13\n",
            "step number  14\n",
            "initial_params= [0.22349722318651266, -0.7121533595290257, 0.7727272829991054, 0.2592863096841016, 0.5455576587399936, 0.7387728671118222, 0.15545123261498978]\n",
            "ðŸ“Š Validation Accuracy (Original Model): 0.8919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Make sure this is at the top if not already imported\n",
        "\n",
        "print(f\"ðŸ“‰ Standard Deviation of Test Accuracies: {np.std(test_accuracies):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85KBDYdqUKzH",
        "outputId": "fe7b03e3-c7a8-47f3-b2d7-e38b8875394a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‰ Standard Deviation of Test Accuracies: 0.0439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZSz76h8VGIZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "\n",
        "class DJGNN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, num_classes, test_indices, initial_preds, n_jumps=5, dropout=0.5):\n",
        "        super(DJGNN, self).__init__()\n",
        "        self.n_jumps = n_jumps\n",
        "        self.dropout = dropout\n",
        "        self.att = nn.Parameter(torch.ones(n_jumps + 1))  # Attention weights for jumps\n",
        "        self.sm = nn.Softmax(dim=0)\n",
        "\n",
        "        # Store initial predictions (converted to one-hot)\n",
        "        self.test_indices = test_indices\n",
        "        self.initial_preds = nn.Parameter(F.one_hot(initial_preds, num_classes).float(), requires_grad=False)\n",
        "\n",
        "        # GCN layers\n",
        "        self.convs = nn.ModuleList()\n",
        "        for _ in range(n_jumps):\n",
        "            self.convs.append(GCNConv(in_dim, hidden_dim))\n",
        "\n",
        "        # Extra convolution layers\n",
        "        self.conv_extra = GCNConv(in_dim, hidden_dim)\n",
        "        self.bn_extra = nn.BatchNorm1d(hidden_dim)\n",
        "        self.conv_extra2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn_extra_2 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "        # Final classifier\n",
        "        self.classify = nn.Linear(hidden_dim * (n_jumps + 1) + num_classes, num_classes)  # Extra input for initial preds\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "      mask_attentions = self.sm(self.att)\n",
        "\n",
        "      # Extra convolutions\n",
        "      extra_conv = self.conv_extra(x, edge_index).relu()\n",
        "      print(f\"extra_conv after conv_extra: {extra_conv.shape}\")\n",
        "      extra_conv = self.bn_extra(extra_conv)\n",
        "      print(f\"extra_conv after bn_extra: {extra_conv.shape}\")\n",
        "      extra_conv = F.dropout(extra_conv, p=self.dropout, training=self.training)\n",
        "      extra_conv = self.conv_extra2(extra_conv, edge_index).relu()\n",
        "      extra_conv = self.bn_extra_2(extra_conv)\n",
        "      extra_conv = extra_conv * mask_attentions[-1]\n",
        "      print(f\"extra_conv after conv_extra2 & mask: {extra_conv.shape}\")\n",
        "\n",
        "      # Jumping knowledge mechanism: collect outputs from each conv layer\n",
        "      z_s = []\n",
        "      for i, conv in enumerate(self.convs):\n",
        "          z = conv(x, edge_index).relu() * mask_attentions[i]\n",
        "          print(f\"z from conv {i}: {z.shape}\")\n",
        "          z_s.append(z)\n",
        "\n",
        "      # Concatenate all jump outputs with extra_conv\n",
        "      final_z = torch.cat(z_s + [extra_conv], dim=1)\n",
        "      print(f\"âœ… final_z shape BEFORE concatenation: {final_z.shape}\")  # Expect [num_nodes, feature_dim]\n",
        "\n",
        "      # Debug: Print test indices shape and the slice from final_z\n",
        "      print(f\"âœ… self.test_indices shape: {self.test_indices.shape}\")  # Expect [num_test_samples]\n",
        "      test_slice = final_z[self.test_indices]\n",
        "      print(f\"âœ… final_z[self.test_indices] shape: {test_slice.shape}\")  # Expect [num_test_samples, feature_dim]\n",
        "\n",
        "      # Ensure self.initial_preds is a LongTensor\n",
        "      num_classes = self.classify.out_features  # Get number of classes\n",
        "      initial_preds_long = self.initial_preds.long().squeeze()\n",
        "      print(f\"initial_preds_long shape after squeeze: {initial_preds_long.shape}\")\n",
        "\n",
        "      # Clamp indices to valid range\n",
        "      initial_preds_long = torch.clamp(initial_preds_long, min=0, max=num_classes - 1)\n",
        "\n",
        "      # Convert to one-hot encoding\n",
        "      one_hot_preds = F.one_hot(initial_preds_long, num_classes=num_classes).float()\n",
        "      print(f\"âœ… one_hot_preds shape immediately after one_hot: {one_hot_preds.shape}\")\n",
        "\n",
        "      # If one_hot_preds still has an extra dimension, print its dimensions explicitly\n",
        "      for idx, dim in enumerate(one_hot_preds.shape):\n",
        "          print(f\"one_hot_preds dim {idx}: {dim}\")\n",
        "\n",
        "      # Attempt to remove extra dimensions by squeezing only the dims of size 1\n",
        "      one_hot_preds = one_hot_preds.squeeze()\n",
        "      print(f\"âœ… one_hot_preds shape after squeeze(): {one_hot_preds.shape}\")\n",
        "\n",
        "      # Check dimensions explicitly\n",
        "      if one_hot_preds.ndim != 2:\n",
        "          print(f\"âŒ one_hot_preds still has {one_hot_preds.ndim} dimensions; expected 2 dimensions!\")\n",
        "\n",
        "      # Now, try concatenation and catch errors with more prints\n",
        "      try:\n",
        "          final_z[self.test_indices] = torch.cat((final_z[self.test_indices], one_hot_preds), dim=1)\n",
        "      except RuntimeError as e:\n",
        "          print(f\"âŒ ERROR: Shape mismatch at concatenation!\")\n",
        "          print(f\"ðŸ” final_z[self.test_indices] shape: {final_z[self.test_indices].shape}\")\n",
        "          print(f\"ðŸ” one_hot_preds shape: {one_hot_preds.shape}\")\n",
        "          raise e  # Re-raise after printing shapes\n",
        "\n",
        "      # Dropout and classification\n",
        "      final_z = F.dropout(final_z, p=self.dropout, training=self.training)\n",
        "      logits = self.classify(final_z).log_softmax(dim=-1)\n",
        "      return logits\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cldUM1jPk72E"
      },
      "outputs": [],
      "source": [
        "def train_djgnn(model, data, train_idx, test_idx, initial_preds_djgnn, epochs=100, lr=0.01):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(data.x, data.edge_index)\n",
        "        loss_supervised = criterion(logits[train_idx], data.y[train_idx])\n",
        "\n",
        "        # ðŸ”¥ NEW: Consistency Loss to prevent drastic changes\n",
        "        confidence_threshold = 0.85  # Change only high-confidence nodes\n",
        "        high_confidence_mask = logits[test_idx].softmax(dim=1).max(dim=1).values > confidence_threshold\n",
        "\n",
        "        # Apply consistency loss only to confident predictions\n",
        "        filtered_loss = F.cross_entropy(logits[test_idx][high_confidence_mask], initial_preds_djgnn[high_confidence_mask])\n",
        "        loss = loss_supervised + 0.1 * filtered_loss\n",
        "\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute accuracy on test set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_logits = model(data.x, data.edge_index)\n",
        "            test_preds = test_logits[test_idx].argmax(dim=1)\n",
        "            test_correct = (test_preds == data.y[test_idx]).sum().item()\n",
        "            test_acc = test_correct / len(test_idx)\n",
        "\n",
        "    #    print(f\"ðŸ§ Epoch {epoch+1:02d} | Loss: {loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "       # if epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "       #     print(f\"ðŸ” Logits Before Argmax (First 5 Samples):\\n{test_logits[test_idx][:5]}\")\n",
        "\n",
        "    return test_preds, test_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p936LZ0GlGf3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Ensure `initial_preds_djgnn` is a tensor and detached\n",
        "initial_preds_djgnn = test_predictions_before_djgnn.clone().detach().long()  # Explicitly convert to LongTensor\n",
        "initial_preds = initial_preds.clone().detach().long()\n",
        "\n",
        "# Get the number of unique classes from `data.y`\n",
        "num_classes = data.y.max().item() + 1  # `max() + 1` ensures it captures all labels\n",
        "\n",
        "print(f\"ðŸ“Š Number of Test Samples: {len(initial_preds_djgnn)}\")\n",
        "print(f\"ðŸ”¢ Number of Unique Classes: {num_classes}\")\n",
        "\n",
        "# ðŸš€ Initialize DJ-GNN model with test indices & initial predictions\n",
        "djgnn_model = DJGNN(\n",
        "    in_dim=data.x.shape[1],\n",
        "    hidden_dim=32,\n",
        "    num_classes=num_classes,  # Use the correct `num_classes`\n",
        "    test_indices=torch.tensor(test_idx, dtype=torch.long, device=data.x.device),  # Ensure `test_idx` is a tensor\n",
        "    initial_preds=initial_preds_djgnn,  # Pass initial predictions (already `LongTensor`)\n",
        "    n_jumps=5,  # Use 5 hops for diffusion\n",
        "    dropout=0.5\n",
        ").to(data.x.device)  # Ensure it's on the correct device\n",
        "\n",
        "# ðŸš€ Train DJ-GNN and get new predictions\n",
        "new_preds, final_accuracy_after_djgnn = train_djgnn(\n",
        "    djgnn_model, data, train_idx, test_idx, initial_preds_djgnn, epochs=500, lr=0.01\n",
        ")\n",
        "\n",
        "# ðŸŽ¯ Compare accuracies before & after DJ-GNN\n",
        "print(\"\\nðŸ”¥ Final Results:\")\n",
        "print(f\"ðŸŽ¯ Accuracy BEFORE DJ-GNN: {final_accuracy_before_djgnn:.4f}\")\n",
        "print(f\"ðŸš€ Accuracy AFTER DJ-GNN: {final_accuracy_after_djgnn:.4f}\")\n",
        "\n",
        "# ðŸ” Sample Predictions Before & After DJ-GNN (Print first 10 for debugging)\n",
        "print(\"\\nðŸ” Sample Predictions Before & After DJ-GNN:\")\n",
        "for i in range(min(10, len(test_idx))):  # Ensure we don't go out of bounds\n",
        "    print(f\"ðŸ§ Sample {i+1} | Before: {initial_preds_djgnn[i].item()} | After: {new_preds[i].item()}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}